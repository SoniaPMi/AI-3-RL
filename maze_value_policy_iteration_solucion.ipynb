{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoniaPMi/AI-3-RL/blob/main/maze_value_policy_iteration_solucion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eX8NewAt-Iq"
      },
      "source": [
        "# Ejemplo del laberinto. Implementación del algoritmo de Evaluación de Políticas.\n",
        "\n",
        "Vamos a revisar esta implementación del ejercicio del laberinto que hemos visto en las diapositivas y que procede del libro de Sutton & Barto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MNchovJt-Iv"
      },
      "outputs": [],
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrZUPkAJt-Iw"
      },
      "source": [
        " Definimos las estructuras de datos:  \n",
        " * Definimos las coordenadas de las posiciones especiales A, B, A', B'. \n",
        "   Ten en cuenta que los índices comienzan por 0 en python\n",
        " * Definimos  la variable DISCOUNT que es nuestra Gamma de teoria a un valor 0.9\n",
        " * La variable WORL_SIZE nos define un tablero o matriz de 5x5\n",
        " * El vector Actions define con dos códigos el cambio de índices en el tablero al moverse \n",
        "   ejemplo: estoy en [2,2], elijo la accion 'left', con  lo que sumo [2,2] + [0,-1] = [2,1] y estoy \n",
        "   una posición a la izquierda\n",
        " * La constante ACTION_PROB es la probabilidad de cada acción\n",
        " \n",
        "    -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D2mFhc-Rt-Ix"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.table import Table\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "WORLD_SIZE = 5\n",
        "A_POS = [0, 1]\n",
        "A_PRIME_POS = [4, 1]\n",
        "B_POS = [0, 3]\n",
        "B_PRIME_POS = [2, 3]\n",
        "DISCOUNT = 0.9\n",
        "\n",
        "# left, up, right, down\n",
        "ACTIONS = [np.array([0, -1]),\n",
        "           np.array([-1, 0]),\n",
        "           np.array([0, 1]),\n",
        "           np.array([1, 0])]\n",
        "ACTIONS_FIGS=[ '←', '↑', '→', '↓']\n",
        "\n",
        "\n",
        "ACTION_PROB = 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoYR2ayRt-Iy"
      },
      "source": [
        "La siguiente función 'step' implementa el paso elemental s_t,a_t,r_{t+1},s_{t+1},a_{t+1}\n",
        "*Si el estado actual es 'A' o 'B', salto a los puntos 'A'' o 'B'' y devuelvo la recompensa +10 ó +5 \n",
        "*Sino, hago el movimiento, sumándole al estado la acción (como hemos explicado antes) y compruebo si\n",
        "me he salido fuera del tablero.\n",
        "    -Si he salido, devuelvo -1 y vuelvo al estado anterior\n",
        "    -Si no he salido, devuelvo 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Xh9bkPet-Iy"
      },
      "outputs": [],
      "source": [
        "def step(state, action):\n",
        "    if state == A_POS:\n",
        "        return A_PRIME_POS, 10\n",
        "    if state == B_POS:\n",
        "        return B_PRIME_POS, 5\n",
        "\n",
        "    next_state = (np.array(state) + action).tolist()\n",
        "    x, y = next_state\n",
        "    if x < 0 or x >= WORLD_SIZE or y < 0 or y >= WORLD_SIZE:\n",
        "        reward = -1.0\n",
        "        next_state = state\n",
        "    else:\n",
        "        reward = 0\n",
        "    return next_state, reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nrsKKCNt-Iz"
      },
      "source": [
        "Las dos funciones (celdas de código) siguientes no tienen interés algorítmico. Son para dibujar la imagen de los resultados del tablero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_qSO_poxt-Iz"
      },
      "outputs": [],
      "source": [
        "def draw_image(image):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_axis_off()\n",
        "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
        "\n",
        "    nrows, ncols = image.shape\n",
        "    width, height = 1.0 / ncols, 1.0 / nrows\n",
        "\n",
        "    # Add cells\n",
        "    for (i, j), val in np.ndenumerate(image):\n",
        "\n",
        "        # add state labels\n",
        "        if [i, j] == A_POS:\n",
        "            val = str(val) + \" (A)\"\n",
        "        if [i, j] == A_PRIME_POS:\n",
        "            val = str(val) + \" (A')\"\n",
        "        if [i, j] == B_POS:\n",
        "            val = str(val) + \" (B)\"\n",
        "        if [i, j] == B_PRIME_POS:\n",
        "            val = str(val) + \" (B')\"\n",
        "        \n",
        "        tb.add_cell(i, j, width, height, text=val,\n",
        "                    loc='center', facecolor='white')\n",
        "        \n",
        "\n",
        "    # Row and column labels...\n",
        "    for i in range(len(image)):\n",
        "        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
        "                    edgecolor='none', facecolor='none')\n",
        "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n",
        "                    edgecolor='none', facecolor='none')\n",
        "\n",
        "    ax.add_table(tb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAaBrXSbt-I0"
      },
      "outputs": [],
      "source": [
        "def draw_policy(optimal_values):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_axis_off()\n",
        "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
        "\n",
        "    nrows, ncols = optimal_values.shape\n",
        "    width, height = 1.0 / ncols, 1.0 / nrows\n",
        "\n",
        "    # Add cells\n",
        "    for (i, j), val in np.ndenumerate(optimal_values):\n",
        "        next_vals=[]\n",
        "        for action in ACTIONS:\n",
        "            next_state, _ = step([i, j], action)\n",
        "            next_vals.append(optimal_values[next_state[0],next_state[1]])\n",
        "\n",
        "        best_actions=np.where(next_vals == np.max(next_vals))[0]\n",
        "        val=''\n",
        "        for ba in best_actions:\n",
        "            val+=ACTIONS_FIGS[ba]\n",
        "        \n",
        "        # add state labels\n",
        "        if [i, j] == A_POS:\n",
        "            val = str(val) + \" (A)\"\n",
        "        if [i, j] == A_PRIME_POS:\n",
        "            val = str(val) + \" (A')\"\n",
        "        if [i, j] == B_POS:\n",
        "            val = str(val) + \" (B)\"\n",
        "        if [i, j] == B_PRIME_POS:\n",
        "            val = str(val) + \" (B')\"\n",
        "        \n",
        "        tb.add_cell(i, j, width, height, text=val,\n",
        "                loc='center', facecolor='white')\n",
        "\n",
        "    # Row and column labels...\n",
        "    for i in range(len(optimal_values)):\n",
        "        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
        "                    edgecolor='none', facecolor='none')\n",
        "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n",
        "                   edgecolor='none', facecolor='none')\n",
        "\n",
        "    ax.add_table(tb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBJNj1wTt-I1"
      },
      "source": [
        "### Este es el meollo\n",
        "La función 'figure_3_2()' implementa el algoritmo 'Iterative Policy Evaluation'\n",
        "La función figure_3_5() implementa el método de optimización del algoritmo 'Value Iteration'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1HHdfRcbt-I1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def figure_3_2():\n",
        "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
        "    while True:\n",
        "        # keep iteration until convergence\n",
        "        new_value = np.zeros_like(value)\n",
        "        for i in range(WORLD_SIZE):\n",
        "            for j in range(WORLD_SIZE):\n",
        "                for action in ACTIONS:\n",
        "                    (next_i, next_j), reward = step([i, j], action)\n",
        "                    # bellman equation\n",
        "                    new_value[i, j] += ACTION_PROB * (reward + DISCOUNT * value[next_i, next_j])\n",
        "        if np.sum(np.abs(value - new_value)) < 1e-4:\n",
        "            draw_image(np.round(new_value, decimals=2))\n",
        "            plt.savefig('imagesfigure_3_2.png')\n",
        "            plt.close()\n",
        "            break\n",
        "        value = new_value\n",
        "\n",
        "\n",
        "\n",
        "def figure_3_5():\n",
        "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
        "    while True:\n",
        "        # keep iteration until convergence\n",
        "        new_value = np.zeros_like(value)\n",
        "        for i in range(WORLD_SIZE):\n",
        "            for j in range(WORLD_SIZE):\n",
        "                values = []\n",
        "                for action in ACTIONS:\n",
        "                    (next_i, next_j), reward = step([i, j], action)\n",
        "                    # value iteration\n",
        "                    values.append(reward + DISCOUNT * value[next_i, next_j])\n",
        "                new_value[i, j] = np.max(values)\n",
        "        if np.sum(np.abs(new_value - value)) < 1e-4:\n",
        "            draw_image(np.round(new_value, decimals=2))\n",
        "            plt.savefig('figure_3_5.png')\n",
        "            plt.close()\n",
        "            draw_policy(new_value)\n",
        "            plt.savefig('figure_3_5_policy.png')\n",
        "            plt.close()\n",
        "            break\n",
        "        value = new_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "21NKr4zCt-I2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-5T4_R64t-I2"
      },
      "outputs": [],
      "source": [
        "def policy_iteration():\n",
        "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
        "    policy = np.random.randint(4,size=(WORLD_SIZE, WORLD_SIZE))  #Inicializo la politica a random\n",
        "    policy_estable = False\n",
        "    while policy_estable == False:\n",
        "        # keep iteration until convergence\n",
        "        new_value = np.zeros_like(value)\n",
        "        for i in range(WORLD_SIZE):\n",
        "            for j in range(WORLD_SIZE):\n",
        "                action = ACTIONS[policy[i,j]]  #pi(s)\n",
        "                (next_i, next_j), reward = step([i, j], action)\n",
        "                    # bellman equation\n",
        "                new_value[i, j] = (reward + DISCOUNT * value[next_i, next_j])\n",
        "        if np.sum(np.abs(value - new_value)) < 1e-4:\n",
        "            #draw_image(np.round(new_value, decimals=2))\n",
        "            #plt.savefig('imagesfigure_3_2.png')\n",
        "            #plt.close()\n",
        "            break\n",
        "        value = new_value\n",
        "      \n",
        "\n",
        "    #Parte de policy_Evaluation para la mejora de la politica\n",
        "    \n",
        "    \n",
        "        # keep iteration until convergence\n",
        "        policy_estable = True\n",
        "        \n",
        "        for i in range(WORLD_SIZE):\n",
        "            for j in range(WORLD_SIZE):\n",
        "                b = policy[i,j]\n",
        "                #Ahora me quedo con la accion que maximiza la ecuacion de Bellman\n",
        "                max_action_value =-1000 #inicializo el valor de accion a uno muy bajo\n",
        "                for action in ACTIONS:\n",
        "                    (next_i, next_j), reward = step([i, j], action)\n",
        "                    aux = (reward + DISCOUNT * value[next_i, next_j])\n",
        "                    if aux > max_action_value:\n",
        "                        max_action_value = aux\n",
        "                        #Guardo en la politica el indice de la accion  máxima {o0, o1, o2, o3}\n",
        "                        policy[i,j] = next((i for i, val in enumerate(ACTIONS) if np.all(val == action)), -1)  \n",
        "                    # bellman equation\n",
        "                    \n",
        "                if b != policy[i,j]:\n",
        "                    policy_estable = False\n",
        "                    \n",
        "       \n",
        "     #volvemos al while inicial               \n",
        "                    \n",
        "    draw_image(np.round(value, decimals=2))\n",
        "    plt.savefig('imagespolicy_iteration.png')\n",
        "    plt.close()\n",
        "    draw_policy(value)\n",
        "    plt.savefig('figure_3_5_policy.png')\n",
        "    plt.close()\n",
        "        \n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TABvgEl0t-I2"
      },
      "source": [
        "### Ejercicio 2.\n",
        "Si te fijas en el resultado que te da el algoritmo implementado \"policy_iteration()\"  de la caja anterior, observarás que,aunque se aproxima al verdadero valor óptimo de cada estado, nollega a alcanzar totalmente el verdadero valor óptimo (compáralo con la diapo 67 de la presentación donde está la solución). Esto es debido a que la parte del algoritmo donde se calcula el valor de V para la política actual no se calcula más que mediante una única iteración sobre los estados, por eficiencia. Modifica esta parte para que sea igual que la función \"figure_3_2()\" que implementa el cálculo más aproximado y compara los resultados que obtienes ahora con el anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NuoAKtIZt-I3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def policy_iteration2():\n",
        "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
        "    policy = np.random.randint(4,size=(WORLD_SIZE, WORLD_SIZE))  #Inicializo la politica a random\n",
        "    policy_estable = False\n",
        "    while policy_estable == False:\n",
        "        # keep iteration until convergence\n",
        "        while True:\n",
        "            new_value = np.zeros_like(value)\n",
        "            for i in range(WORLD_SIZE):\n",
        "                for j in range(WORLD_SIZE):\n",
        "                    action = ACTIONS[policy[i,j]]  #pi(s)\n",
        "                    (next_i, next_j), reward = step([i, j], action)\n",
        "                    # bellman equation\n",
        "                    new_value[i, j] = (reward + DISCOUNT * value[next_i, next_j])\n",
        "            if np.sum(np.abs(value - new_value)) < 1e-4:\n",
        "            #draw_image(np.round(new_value, decimals=2))\n",
        "            #plt.savefig('imagesfigure_3_2.png')\n",
        "            #plt.close()\n",
        "                break\n",
        "            value = new_value\n",
        "      \n",
        "\n",
        "    #Parte de policy_Evaluation para la mejora de la politica\n",
        "    \n",
        "    \n",
        "        # keep iteration until convergence\n",
        "        policy_estable = True\n",
        "        \n",
        "        for i in range(WORLD_SIZE):\n",
        "            for j in range(WORLD_SIZE):\n",
        "                b = policy[i,j]\n",
        "                #Ahora me quedo con la accion que maximiza la ecuacion de Bellman\n",
        "                max_action_value =-1000 #inicializo el valor de accion a uno muy bajo\n",
        "                for action in ACTIONS:\n",
        "                    (next_i, next_j), reward = step([i, j], action)\n",
        "                    aux = (reward + DISCOUNT * value[next_i, next_j])\n",
        "                    if aux > max_action_value:\n",
        "                        max_action_value = aux\n",
        "                        #Guardo en la politica el indice de la accion  máxima {o0, o1, o2, o3}\n",
        "                        policy[i,j] = next((i for i, val in enumerate(ACTIONS) if np.all(val == action)), -1)  \n",
        "                    # bellman equation\n",
        "                    \n",
        "                if b != policy[i,j]:\n",
        "                    policy_estable = False\n",
        "                    \n",
        "       \n",
        "     #volvemos al while inicial               \n",
        "                    \n",
        "    draw_image(np.round(value, decimals=2))\n",
        "    plt.savefig('imagespolicy_iteration_ejercicio2.png')\n",
        "    plt.close()\n",
        "    draw_policy(value)\n",
        "    plt.savefig('figure_3_5_policy2_ejercicio2.png')\n",
        "    plt.close()\n",
        "        \n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M7guHyJct-I3"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "   \n",
        "    #figure_3_2()\n",
        "    #figure_3_5()\n",
        "    policy_iteration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eQsT7QPFt-I3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "maze_value_policy_iteration_solucion.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}