{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3C_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoniaPMi/AI-3-RL/blob/main/A3C_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWsxkki8jh8A"
      },
      "source": [
        "**texto en negrita**# A3C v1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSeTTLadjXBg"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "import datetime\n",
        "from multiprocessing import cpu_count\n",
        "import time\n",
        "import threading\n",
        "from typing import Optional\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udd9Gvt78b-2"
      },
      "source": [
        "## Utils function:\n",
        "Transform discrete states into one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpIqeiKm8ync"
      },
      "source": [
        "def discrete_input(state_discrete: tuple, env_dim: gym.spaces.tuple.Tuple):\n",
        "    one_hot_state = []\n",
        "    for i_pos, dim in zip(state_discrete, env_dim):\n",
        "        temp = np.zeros(dim.n)\n",
        "        temp[i_pos] = 1\n",
        "        one_hot_state.append(temp)\n",
        "\n",
        "    return np.concatenate(one_hot_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek0pG57w82jN"
      },
      "source": [
        "## A3C aproximator\n",
        "This is the neural network for A3C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZEY5iHJ9BKy"
      },
      "source": [
        "\n",
        "class DenseNetA3C(Model):\n",
        "    def get_config(self):\n",
        "        pass\n",
        "\n",
        "    def __init__(self, action_size, num_units=50, custom_env: bool = False):\n",
        "        super(DenseNetA3C, self).__init__(name='a3c_dense')\n",
        "        self.action_size = action_size\n",
        "        self.dense1 = layers.Dense(num_units, activation='relu')\n",
        "        self.policy = layers.Dense(action_size, activation='softmax')\n",
        "        self.dense2 = layers.Dense(num_units, activation='relu')\n",
        "        self.values = layers.Dense(1)\n",
        "        self.custom_env = custom_env\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        # Reshape input to allow input sequences into the fully connected network\n",
        "        if self.custom_env:\n",
        "            inputs = tf.reshape(inputs, [-1, inputs.shape[1] * inputs.shape[2]])\n",
        "\n",
        "        # Forward pass\n",
        "        x = self.dense1(inputs)\n",
        "        policy_prob = self.policy(x)\n",
        "        v1 = self.dense2(inputs)\n",
        "        values = self.values(v1)\n",
        "        return policy_prob, values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tQCzr628Xxg"
      },
      "source": [
        "## A3C AGENT\n",
        "This sections define the classes for the agent to operate with threading.\n",
        "Each agent learns from its own experiences and then updates/synchronizes the learning with the global network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2wbtvIs8lor"
      },
      "source": [
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def store(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "\n",
        "class A3CAgent(threading.Thread):\n",
        "    # Set up global variables across different threads\n",
        "    stop_signal = False\n",
        "    global_episode = 0\n",
        "    global_ema_reward = None\n",
        "    best_score = None\n",
        "    lock = threading.Lock()\n",
        "\n",
        "    def __init__(self, env, local_model, opt, idx,\n",
        "                 max_episodes=1000, update_freq=20, gamma=.99,\n",
        "                 grad_prop: bool = False, normalize: bool = True,\n",
        "                 c_actor: float = 1, c_critic: float = 0.5, c_entropy: float = 0.01,\n",
        "                 ema_ratio: float = 0.01, early_stopping=None, max_steps_per_episode=200):\n",
        "        super(A3CAgent, self).__init__()\n",
        "        self.env = env\n",
        "        self.local_model = local_model\n",
        "        self.global_model = None\n",
        "        self.opt = opt\n",
        "        self.worker_idx = idx\n",
        "        self.max_episodes = max_episodes\n",
        "        self.update_freq = update_freq\n",
        "        self.gamma = gamma\n",
        "        self.name = ''\n",
        "        self.folder_model = None\n",
        "        self.summary_writer = None\n",
        "        self.memory = Memory()\n",
        "        self.eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n",
        "        self.grad_prop: bool = grad_prop\n",
        "        self.normalize: bool = normalize\n",
        "        self.c_actor: float = c_actor\n",
        "        self.c_critic: float = c_critic\n",
        "        self.c_entropy: float = c_entropy\n",
        "        self.ema_ratio: float = ema_ratio\n",
        "        self.max_steps_per_episode: int = max_steps_per_episode\n",
        "        self.early_stopping: Optional[float] = early_stopping\n",
        "\n",
        "    def __record_results(self, episode_reward, worker_idx, episode_loss_mean, num_steps):\n",
        "        \"\"\"Helper function to store score and print statistics.\n",
        "\n",
        "        Arguments:\n",
        "          episode_reward: Reward accumulated over the current episode\n",
        "          worker_idx: Which thread (worker)\n",
        "          episode_loss_mean: The total loss accumulated over the current episode\n",
        "          num_steps: The number of steps the episode took to complete\n",
        "        \"\"\"\n",
        "\n",
        "        # Save Tensorboard\n",
        "        with self.summary_writer.as_default():\n",
        "            tf.summary.scalar('Episode Reward', episode_reward, step=A3CAgent.global_episode)\n",
        "            tf.summary.scalar('Agent ID', self.worker_idx, step=A3CAgent.global_episode)\n",
        "            tf.summary.scalar('Episode EMA Reward', A3CAgent.global_ema_reward, step=A3CAgent.global_episode)\n",
        "            tf.summary.scalar('Episode Loss', episode_loss_mean, step=A3CAgent.global_episode)\n",
        "\n",
        "        # Print in command line\n",
        "        print(f\"Episode: ({A3CAgent.global_episode + 1}/{self.max_episodes}) | \"\n",
        "              f\"EMA Reward: {A3CAgent.global_ema_reward:.3} | \"\n",
        "              f\"Reward: {episode_reward:.3} | \"\n",
        "              f\"Loss: {episode_loss_mean:.3} | \"\n",
        "              f\"Steps: {num_steps} | Worker: {worker_idx}\")\n",
        "\n",
        "    def __predict_action(self, current_state):\n",
        "        probs, _ = self.local_model(tf.convert_to_tensor(current_state[None, :], dtype=tf.float32))\n",
        "        action = np.random.choice(self.env.action_size, p=probs.numpy()[0])\n",
        "        return action\n",
        "\n",
        "    @staticmethod\n",
        "    def __update_ema_reward(episode_reward, ema_ratio=0.01):\n",
        "        if A3CAgent.global_ema_reward is None:\n",
        "            A3CAgent.global_ema_reward = episode_reward\n",
        "        else:\n",
        "            A3CAgent.global_ema_reward = A3CAgent.global_ema_reward * (1 - ema_ratio) + episode_reward * ema_ratio\n",
        "\n",
        "    def __compute_loss(self, done, new_state):\n",
        "        if done:\n",
        "            reward_sum = 0.  # terminal\n",
        "        else:\n",
        "            _, values_pred = self.local_model(tf.convert_to_tensor(new_state[None, :], dtype=tf.float32))\n",
        "            reward_sum = values_pred.numpy()[0].item()\n",
        "\n",
        "        # Estimate discounted reward for every step (matrix computation)\n",
        "        rewards = self.memory.rewards + [reward_sum]\n",
        "        gamma_n = np.power(self.gamma, np.arange(len(rewards)))\n",
        "        returns = (rewards * gamma_n)[::-1].cumsum()[::-1] / gamma_n\n",
        "        returns = returns[:-1]\n",
        "\n",
        "        # Predict policy probabilities and values for given states\n",
        "        states = np.stack(self.memory.states)\n",
        "        policy_probs, values = self.local_model(tf.convert_to_tensor(states, dtype=tf.float32))\n",
        "\n",
        "        # Normalize rewards to obtain more stability\n",
        "        if self.normalize:\n",
        "            returns = np.array(returns)\n",
        "            returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
        "            # returns = returns.tolist()\n",
        "\n",
        "        # Estimate advantages\n",
        "        advantage = tf.convert_to_tensor(returns[:, None], dtype=tf.float32) - values\n",
        "\n",
        "        # Value loss\n",
        "        value_loss = tf.square(advantage)\n",
        "\n",
        "        # Policy loss\n",
        "        actions = tf.one_hot(self.memory.actions, depth=self.env.action_size)\n",
        "        pi = tf.reduce_sum(policy_probs * actions, axis=1, keepdims=True)  # select probs for given actions\n",
        "        if self.grad_prop:\n",
        "            policy_loss = - tf.math.log(pi + self.eps) * tf.stop_gradient(advantage)\n",
        "        else:\n",
        "            policy_loss = - tf.math.log(pi + self.eps) * advantage\n",
        "\n",
        "        # Entropy to regularize loss\n",
        "        entropy_policy = tf.reduce_sum(policy_probs * tf.math.log(policy_probs + self.eps), axis=1, keepdims=True)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = tf.reduce_mean(self.c_actor * policy_loss +\n",
        "                                    self.c_critic * value_loss +\n",
        "                                    self.c_entropy * entropy_policy)\n",
        "        return total_loss\n",
        "\n",
        "    def __train(self, done, new_state):\n",
        "        # Calculate gradient wrt to local model. We do so by tracking the\n",
        "        # variables involved in computing the loss by using tf.GradientTape\n",
        "        with tf.GradientTape() as tape:\n",
        "            total_loss = self.__compute_loss(done, new_state)\n",
        "\n",
        "        # Calculate local gradients\n",
        "        grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
        "\n",
        "        # Push local gradients to global model\n",
        "        self.opt.apply_gradients(zip(grads, self.global_model.trainable_weights))\n",
        "\n",
        "        # Update local model with new weights\n",
        "        self.local_model.set_weights(self.global_model.get_weights())\n",
        "        return total_loss.numpy()\n",
        "\n",
        "    def __save_model(self):\n",
        "        print(f'Saving best model to {self.folder_model}, episode score: {A3CAgent.best_score}')\n",
        "        if not os.path.exists(self.folder_model):\n",
        "            os.makedirs(self.folder_model)\n",
        "        self.global_model.save_weights(os.path.join(self.folder_model, f'{self.name}.h5'))\n",
        "\n",
        "    def __run_episodes(self):\n",
        "        while A3CAgent.global_episode < self.max_episodes and not self.early_stop():\n",
        "            current_state = self.env.reset()\n",
        "            self.memory.clear()\n",
        "            ep_reward = 0.\n",
        "            ep_steps = 0\n",
        "            ep_losses = 0\n",
        "\n",
        "            time_count = 0\n",
        "            done = False\n",
        "            while not done or ep_steps > self.max_steps_per_episode:\n",
        "                action = self.__predict_action(current_state)\n",
        "                new_state, reward, done, _ = self.env.step(action)\n",
        "                # if isinstance(self.env.observation_space, gym.spaces.tuple.Tuple):\n",
        "                #     new_state = discrete_input(new_state, self.env.observation_space)\n",
        "                ep_reward += reward\n",
        "                self.memory.store(current_state, action, reward)\n",
        "\n",
        "                if time_count == self.update_freq or done:\n",
        "                    total_loss = self.__train(done, new_state)\n",
        "                    self.memory.clear()\n",
        "                    ep_losses += total_loss\n",
        "                    time_count = 0\n",
        "\n",
        "                    if done or ep_steps > self.max_steps_per_episode:\n",
        "                        with A3CAgent.lock:\n",
        "                            # Stop all the threads if max_episodes reached\n",
        "                            if A3CAgent.global_episode >= self.max_episodes:\n",
        "                                A3CAgent.stop_signal = True\n",
        "                                break\n",
        "\n",
        "                            # Average episode loss\n",
        "                            episode_loss_mean = ep_losses / ep_steps\n",
        "\n",
        "                            # Instantiate best_score with the first ep_reward.\n",
        "                            if A3CAgent.best_score is None:\n",
        "                                A3CAgent.best_score = ep_reward\n",
        "                            #  Re-estimate moving average of the episode reward: A3CAgent.global_ema_reward\n",
        "                            self.__update_ema_reward(ep_reward, ema_ratio=self.ema_ratio)\n",
        "\n",
        "                            # Print Info and write Tensorboard\n",
        "                            self.__record_results(ep_reward, self.worker_idx, episode_loss_mean, ep_steps)\n",
        "\n",
        "                            # Save model only if it is better than the already stored.\n",
        "                            if ep_reward > A3CAgent.best_score:\n",
        "                                A3CAgent.best_score = ep_reward\n",
        "                                self.__save_model()\n",
        "\n",
        "                            if ep_steps > 100 and self.early_stop():\n",
        "                                print(\"Solved at episode {}!\".format(A3CAgent.global_episode))\n",
        "                                A3CAgent.stop_signal = True\n",
        "                                break\n",
        "\n",
        "                            A3CAgent.global_episode += 1\n",
        "\n",
        "                ep_steps += 1\n",
        "                time_count += 1\n",
        "                current_state = new_state\n",
        "\n",
        "    def run(self):\n",
        "        self.__run_episodes()\n",
        "        A3CAgent.stop_signal = True\n",
        "\n",
        "    # def stop(self):\n",
        "    #     while True:\n",
        "    #         if A3CAgent.stop_signal is True:\n",
        "    #             break\n",
        "\n",
        "    def early_stop(self):\n",
        "        if self.early_stopping is not None and \\\n",
        "                A3CAgent.global_ema_reward is not None and \\\n",
        "                A3CAgent.global_ema_reward > self.early_stopping:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdt686u4_2XY"
      },
      "source": [
        "## A3C Runner\n",
        "This is the section where the agents are coordinated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRsND1xV_15a"
      },
      "source": [
        "\n",
        "class A3CRunner:\n",
        "    def __init__(self, agent, global_model, name, folder_model, summary_writer):\n",
        "        self.agent = agent\n",
        "        self.global_model = global_model\n",
        "\n",
        "        # # Build directories\n",
        "        # run_date = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        # folder_model = os.path.join('./models_a3c', name, run_date)\n",
        "        # self.folder_tensor_board = os.path.join('./logs_tensorboard_a3c', name, run_date)\n",
        "\n",
        "        # if not os.path.exists(self.folder_tensor_board):\n",
        "        #     os.makedirs(self.folder_tensor_board)\n",
        "\n",
        "        # # Tensorboard summary writer\n",
        "        # summary_writer = tf.summary.create_file_writer(self.folder_tensor_board)\n",
        "\n",
        "        # Transfer info to every worker\n",
        "        for worker in self.agent:\n",
        "            worker.global_model = self.global_model\n",
        "            worker.summary_writer = summary_writer\n",
        "            worker.folder_model = folder_model\n",
        "            worker.name = name\n",
        "\n",
        "        # Initialize global model to get weights later\n",
        "        rand_state = self.agent[0].env.reset()[None, :]\n",
        "        self.global_model(tf.convert_to_tensor(rand_state, dtype=tf.float32))\n",
        "\n",
        "    def run(self):\n",
        "        if A3CAgent.global_ema_reward is None:\n",
        "            [worker.start() for worker in self.agent]\n",
        "        else:\n",
        "            A3CAgent.stop_signal = False\n",
        "            A3CAgent.global_ema_reward = None\n",
        "            [worker.run() for worker in self.agent]\n",
        "\n",
        "        while True:\n",
        "            if self.agent[0].stop_signal is True:\n",
        "                break\n",
        "\n",
        "        [w.join() for w in self.agent]\n",
        "\n",
        "    def play(self, plays=100, max_steps_per_episode=100, show_plots=False):\n",
        "        env = self.agent[0].env\n",
        "\n",
        "        episode_rewards, episode_steps, entropy = [], [], []\n",
        "        for plays_i in range(plays):\n",
        "            current_state = env.reset()\n",
        "            ep_reward = 0.\n",
        "            ep_steps = 0\n",
        "            done = False\n",
        "            while not done or ep_steps > max_steps_per_episode:\n",
        "                # action = self.agent[0].__predict_action(current_state)\n",
        "                probs, _ = self.global_model(tf.convert_to_tensor(current_state[None, :], dtype=tf.float32))\n",
        "                action = np.random.choice(env.action_size, p=probs.numpy()[0])\n",
        "\n",
        "                entropy_i = probs.numpy()\n",
        "                entropy.append(np.sum(-entropy_i * np.log(entropy_i)))\n",
        "\n",
        "                new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                ep_reward += reward\n",
        "                ep_steps += 1\n",
        "                current_state = new_state\n",
        "            episode_rewards.append(ep_reward)\n",
        "            episode_steps.append(ep_steps)\n",
        "\n",
        "        print(f\"Avg rewards: {np.mean(episode_rewards).round(2)} +/- {np.std(episode_rewards).round(2)}\")\n",
        "        print(f\"Avg steps: {np.mean(episode_steps).round(2)} +/- {np.std(episode_steps).round(2)}\")\n",
        "        print(f\"Avg entropy: {np.mean(entropy).round(2)} +/- {np.std(entropy).round(2)}\")\n",
        "\n",
        "        if show_plots:\n",
        "            num_bins = 50\n",
        "            x = np.array(episode_rewards)\n",
        "            fig, ax = plt.subplots()\n",
        "\n",
        "            # the histogram of the data\n",
        "            n, bins, patches = ax.hist(x, num_bins, density=1)\n",
        "\n",
        "            ax.set_xlabel('Episode rewards')\n",
        "            ax.set_ylabel('Probability density')\n",
        "            ax.set_title(f'Mean {np.mean(x).round(2)} +/- {np.std(x).round(2)}')\n",
        "\n",
        "            # Tweak spacing to prevent clipping of ylabel\n",
        "            fig.tight_layout()\n",
        "            plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn0-uTtqkPeC"
      },
      "source": [
        "Disable GPU computation for local devices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-dueBMmj8QY"
      },
      "source": [
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gvh2VHsje3w"
      },
      "source": [
        "## Gym selection and basic configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A722SC0km68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc1863a-903e-495b-f56f-ecfa77aff0d5"
      },
      "source": [
        "gym_name_list = [\n",
        "    {\n",
        "        'name': 'CartPole-v0',\n",
        "        'goal': 180,\n",
        "        'v_min': 0,\n",
        "        'v_max': 210,\n",
        "        'ep': 50\n",
        "    },\n",
        "    {\n",
        "        'name': 'MountainCar-v0',\n",
        "        'goal': -150, \n",
        "        'v_min': -210,\n",
        "        'v_max': 0,\n",
        "        'ep': 20\n",
        "    },\n",
        "    {\n",
        "        'name': 'Blackjack-v0',\n",
        "        'goal': 0.10,\n",
        "        'v_min': -20,\n",
        "        'v_max': 20,\n",
        "        'ep': 1000\n",
        "    }\n",
        "]\n",
        "\n",
        "env_i = 2 #@param {type:\"slider\", min:0, max:2, step:1}\n",
        "\n",
        "save_model: bool = False # @param {type:\"boolean\"}\n",
        "show_plots: bool = True # @param {type:\"boolean\"}\n",
        "\n",
        "# The code fails when environment is rendered in COLAB.\n",
        "render_env: bool = False # @param {type:\"boolean\"}\n",
        "seed = 42 # @param {type:\"integer\"}\n",
        "\n",
        "max_steps_per_episode = 200 # @param {type:\"integer\"}\n",
        "\n",
        "print(f\"These are the CPU cores available: {cpu_count()}\")\n",
        "N_WORKERS: int = 2 # @param {type:\"integer\"}\n",
        "\n",
        "stopping_reward_criteria = gym_name_list[env_i]['goal']\n",
        "\n",
        "gym_name = gym_name_list[env_i]['name']\n",
        "\n",
        "envs = []\n",
        "for _ in range(N_WORKERS):\n",
        "    env = gym.make(gym_name)  # Create the environment\n",
        "    if isinstance(env.observation_space, gym.spaces.tuple.Tuple):\n",
        "        env = gym.wrappers.TransformObservation(env, lambda obs: discrete_input(obs, env.observation_space))\n",
        "    env.seed(seed)\n",
        "    env.action_size = env.action_space.n\n",
        "    envs.append(env)\n",
        "\n",
        "if isinstance(envs[0].observation_space, gym.spaces.tuple.Tuple):\n",
        "    num_inputs = sum([x.n for x in envs[0].observation_space])\n",
        "else:\n",
        "    num_inputs = envs[0].observation_space.shape[0]\n",
        "num_actions = envs[0].action_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These are the CPU cores available: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9RpNDIeeK6-",
        "outputId": "65ec6651-00e3-4187-e8cb-db307b58f448"
      },
      "source": [
        "envs[0].observation_space\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tuple(Discrete(32), Discrete(11), Discrete(2))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWXm85B4lIJ7"
      },
      "source": [
        "## Algorithm hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqDFkMCflFY-"
      },
      "source": [
        "# Factor of the ema that displays that tracks the averaged rewards\n",
        "ema_ratio = 0.01  # @param {type:\"number\"}\n",
        "\n",
        "# Maximum steps in a episode\n",
        "max_steps_per_episode: int = 200 # @param  {type:\"integer\"}\n",
        "\n",
        "# Discount factor for estimating the futures rewards\n",
        "gamma: float = 0.99  # @param {type:\"number\"}\n",
        "\n",
        "# Decide if normalize the episode returns\n",
        "normalize: bool = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Decide if apply gradient propagation through critic value, in theory NOT.\n",
        "grad_prop: bool = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# Weights in applied loss components\n",
        "c_actor = 1.0  # @param {type:\"number\"}\n",
        "c_critic = 0.5  # @param {type:\"number\"}\n",
        "c_entropy = 0.01  # @param {type:\"number\"}\n",
        "\n",
        "# The usual factor that controls the amount of change the weights are updated\n",
        "learning_rate = 0.001 # @param {type:\"number\"}\n",
        "\n",
        "# Factor to define heuristically the size of the hidden layer.\n",
        "hidden_size_factor = 16 # @param {type:\"integer\"}\n",
        "num_hidden = num_inputs * num_actions * hidden_size_factor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2H04rFNqdNg"
      },
      "source": [
        "## Tensorboard configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyP6U0yKq-Ez"
      },
      "source": [
        "implementation = \"A3C_v0\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = os.path.join(\"logs\", gym_name, implementation, \"T_\" + current_time)\n",
        "summary_writer = tf.summary.create_file_writer(train_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "VkJuXmNph9wb",
        "outputId": "7cad842f-d195-40dc-c800-b704d846e563"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "\n",
        "# from tensorboard import notebook\n",
        "# notebook.list() # View open TensorBoard instances\n",
        "\n",
        "# # Control TensorBoard display. If no port is provided, \n",
        "# # the most recently launched TensorBoard is used\n",
        "# notebook.display(port=6006, height=1000) \n",
        "\n",
        "%tensorboard --logdir ./logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 144), started 0:16:11 ago. (Use '!kill 144' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2FEzyMp9l6-"
      },
      "source": [
        "## Main function (script) to activate the learning:\n",
        "The logic is in the A3C agent and runner. This just instantiate the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJvX4Zxs-X4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e77d68-4fc3-4ca5-ba44-243456127c18"
      },
      "source": [
        "\n",
        "# OPT = tf.keras.optimizers.Adam(learning_rate=learning_rate, use_locking=True)\n",
        "OPT = tf.compat.v1.train.AdamOptimizer(learning_rate, use_locking=True)\n",
        "\n",
        "# Neural approximator\n",
        "neural_nets = [DenseNetA3C(action_size=envs[0].action_size,\n",
        "                           num_units=num_hidden,\n",
        "                           custom_env=False) for _ in range(N_WORKERS + 1)]\n",
        "\n",
        "\n",
        "# Agent\n",
        "workers = [A3CAgent(env=envs[i],\n",
        "                    local_model=neural_nets[i],\n",
        "                    opt=OPT,\n",
        "                    idx=i,\n",
        "                    max_episodes=15000,\n",
        "                    update_freq=20,\n",
        "                    gamma=gamma,\n",
        "                    grad_prop=grad_prop,\n",
        "                    c_actor=c_actor,\n",
        "                    c_critic=c_critic,\n",
        "                    c_entropy=c_entropy,\n",
        "                    ema_ratio=ema_ratio,\n",
        "                    early_stopping=stopping_reward_criteria,\n",
        "                    max_steps_per_episode=max_steps_per_episode) for i in range(N_WORKERS)]\n",
        "\n",
        "# Runner\n",
        "runner = A3CRunner(agent=workers,\n",
        "                   global_model=neural_nets[-1],\n",
        "                   name=implementation,\n",
        "                   folder_model=train_log_dir,\n",
        "                   summary_writer=summary_writer)\n",
        "\n",
        "runner.run()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:191: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: (1/15000) | EMA Reward: -1.0 | Reward: -1.0 | Loss: inf | Steps: 0 | Worker: 1\n",
            "Episode: (2/15000) | EMA Reward: -0.98 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Saving best model to logs/Blackjack-v0/A3C_v0/T_20220309-003000, episode score: 1.0\n",
            "Episode: (3/15000) | EMA Reward: -0.96 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (4/15000) | EMA Reward: -0.961 | Reward: -1.0 | Loss: 0.346 | Steps: 1 | Worker: 1\n",
            "Episode: (5/15000) | EMA Reward: -0.941 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (6/15000) | EMA Reward: -0.922 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (7/15000) | EMA Reward: -0.922 | Reward: -1.0 | Loss: 0.415 | Steps: 1 | Worker: 0\n",
            "Episode: (8/15000) | EMA Reward: -0.923 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (9/15000) | EMA Reward: -0.904 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (10/15000) | EMA Reward: -0.905 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (11/15000) | EMA Reward: -0.886 | Reward: 1.0 | Loss: 0.262 | Steps: 2 | Worker: 1\n",
            "Episode: (12/15000) | EMA Reward: -0.887 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (13/15000) | EMA Reward: -0.868 | Reward: 1.0 | Loss: 0.363 | Steps: 1 | Worker: 0\n",
            "Episode: (14/15000) | EMA Reward: -0.869 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (15/15000) | EMA Reward: -0.871 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (16/15000) | EMA Reward: -0.872 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (17/15000) | EMA Reward: -0.873 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (18/15000) | EMA Reward: -0.875 | Reward: -1.0 | Loss: 0.0999 | Steps: 2 | Worker: 1\n",
            "Episode: (19/15000) | EMA Reward: -0.876 | Reward: -1.0 | Loss: 0.215 | Steps: 1 | Worker: 0\n",
            "Episode: (20/15000) | EMA Reward: -0.877 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (21/15000) | EMA Reward: -0.878 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (22/15000) | EMA Reward: -0.88 | Reward: -1.0 | Loss: 0.0487 | Steps: 1 | Worker: 1\n",
            "Episode: (23/15000) | EMA Reward: -0.861 | Reward: 1.0 | Loss: 0.181 | Steps: 3 | Worker: 0\n",
            "Episode: (24/15000) | EMA Reward: -0.862 | Reward: -1.0 | Loss: 0.187 | Steps: 2 | Worker: 1\n",
            "Episode: (25/15000) | EMA Reward: -0.863 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (26/15000) | EMA Reward: -0.865 | Reward: -1.0 | Loss: 0.0655 | Steps: 1 | Worker: 1\n",
            "Episode: (27/15000) | EMA Reward: -0.846 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (28/15000) | EMA Reward: -0.828 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (29/15000) | EMA Reward: -0.809 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (30/15000) | EMA Reward: -0.811 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (31/15000) | EMA Reward: -0.813 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (32/15000) | EMA Reward: -0.795 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (33/15000) | EMA Reward: -0.797 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (34/15000) | EMA Reward: -0.779 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (35/15000) | EMA Reward: -0.781 | Reward: -1.0 | Loss: 0.479 | Steps: 1 | Worker: 1\n",
            "Episode: (36/15000) | EMA Reward: -0.764 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (37/15000) | EMA Reward: -0.766 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (38/15000) | EMA Reward: -0.768 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (39/15000) | EMA Reward: -0.771 | Reward: -1.0 | Loss: 0.0612 | Steps: 1 | Worker: 0\n",
            "Episode: (40/15000) | EMA Reward: -0.773 | Reward: -1.0 | Loss: 0.311 | Steps: 1 | Worker: 0\n",
            "Episode: (41/15000) | EMA Reward: -0.775 | Reward: -1.0 | Loss: 0.0962 | Steps: 1 | Worker: 1\n",
            "Episode: (42/15000) | EMA Reward: -0.777 | Reward: -1.0 | Loss: 0.112 | Steps: 2 | Worker: 0\n",
            "Episode: (43/15000) | EMA Reward: -0.78 | Reward: -1.0 | Loss: -0.00439 | Steps: 1 | Worker: 1\n",
            "Episode: (44/15000) | EMA Reward: -0.782 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (45/15000) | EMA Reward: -0.764 | Reward: 1.0 | Loss: 0.228 | Steps: 2 | Worker: 0\n",
            "Episode: (46/15000) | EMA Reward: -0.766 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (47/15000) | EMA Reward: -0.769 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (48/15000) | EMA Reward: -0.751 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (49/15000) | EMA Reward: -0.744 | Reward: 0.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (50/15000) | EMA Reward: -0.746 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (51/15000) | EMA Reward: -0.729 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (52/15000) | EMA Reward: -0.731 | Reward: -1.0 | Loss: 0.313 | Steps: 1 | Worker: 1\n",
            "Episode: (53/15000) | EMA Reward: -0.734 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (54/15000) | EMA Reward: -0.737 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (55/15000) | EMA Reward: -0.739 | Reward: -1.0 | Loss: inf | Steps: 0 | Worker: 0\n",
            "Episode: (56/15000) | EMA Reward: -0.742 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (57/15000) | EMA Reward: -0.745 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (58/15000) | EMA Reward: -0.747 | Reward: -1.0 | Loss: inf | Steps: 0 | Worker: 1\n",
            "Episode: (59/15000) | EMA Reward: -0.75 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (60/15000) | EMA Reward: -0.752 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (61/15000) | EMA Reward: -0.755 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (62/15000) | EMA Reward: -0.757 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (63/15000) | EMA Reward: -0.739 | Reward: 1.0 | Loss: 0.625 | Steps: 1 | Worker: 0\n",
            "Episode: (64/15000) | EMA Reward: -0.732 | Reward: 0.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (65/15000) | EMA Reward: -0.715 | Reward: 1.0 | Loss: 0.649 | Steps: 1 | Worker: 0\n",
            "Episode: (66/15000) | EMA Reward: -0.698 | Reward: 1.0 | Loss: 0.634 | Steps: 1 | Worker: 1\n",
            "Episode: (67/15000) | EMA Reward: -0.701 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (68/15000) | EMA Reward: -0.684 | Reward: 1.0 | Loss: 0.512 | Steps: 1 | Worker: 1\n",
            "Episode: (69/15000) | EMA Reward: -0.687 | Reward: -1.0 | Loss: inf | Steps: 0 | Worker: 1\n",
            "Episode: (70/15000) | EMA Reward: -0.69 | Reward: -1.0 | Loss: -0.021 | Steps: 2 | Worker: 0\n",
            "Episode: (71/15000) | EMA Reward: -0.673 | Reward: 1.0 | Loss: 0.401 | Steps: 1 | Worker: 0\n",
            "Episode: (72/15000) | EMA Reward: -0.676 | Reward: -1.0 | Loss: 0.131 | Steps: 3 | Worker: 1\n",
            "Episode: (73/15000) | EMA Reward: -0.66 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (74/15000) | EMA Reward: -0.643 | Reward: 1.0 | Loss: 0.312 | Steps: 1 | Worker: 1\n",
            "Episode: (75/15000) | EMA Reward: -0.627 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (76/15000) | EMA Reward: -0.61 | Reward: 1.0 | Loss: 0.638 | Steps: 1 | Worker: 1\n",
            "Episode: (77/15000) | EMA Reward: -0.594 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (78/15000) | EMA Reward: -0.598 | Reward: -1.0 | Loss: 0.731 | Steps: 1 | Worker: 0\n",
            "Episode: (79/15000) | EMA Reward: -0.602 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (80/15000) | EMA Reward: -0.606 | Reward: -1.0 | Loss: 0.237 | Steps: 1 | Worker: 1\n",
            "Episode: (81/15000) | EMA Reward: -0.61 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (82/15000) | EMA Reward: -0.594 | Reward: 1.0 | Loss: 0.349 | Steps: 1 | Worker: 1\n",
            "Episode: (83/15000) | EMA Reward: -0.598 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (84/15000) | EMA Reward: -0.602 | Reward: -1.0 | Loss: 0.178 | Steps: 2 | Worker: 1\n",
            "Episode: (85/15000) | EMA Reward: -0.606 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (86/15000) | EMA Reward: -0.61 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (87/15000) | EMA Reward: -0.594 | Reward: 1.0 | Loss: 0.459 | Steps: 1 | Worker: 0\n",
            "Episode: (88/15000) | EMA Reward: -0.578 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (89/15000) | EMA Reward: -0.582 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (90/15000) | EMA Reward: -0.566 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (91/15000) | EMA Reward: -0.551 | Reward: 1.0 | Loss: 0.217 | Steps: 1 | Worker: 1\n",
            "Episode: (92/15000) | EMA Reward: -0.555 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (93/15000) | EMA Reward: -0.56 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (94/15000) | EMA Reward: -0.544 | Reward: 1.0 | Loss: inf | Steps: 0 | Worker: 0\n",
            "Episode: (95/15000) | EMA Reward: -0.549 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (96/15000) | EMA Reward: -0.553 | Reward: -1.0 | Loss: inf | Steps: 0 | Worker: 0\n",
            "Episode: (97/15000) | EMA Reward: -0.558 | Reward: -1.0 | Loss: -0.183 | Steps: 1 | Worker: 1\n",
            "Episode: (98/15000) | EMA Reward: -0.562 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (99/15000) | EMA Reward: -0.566 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (100/15000) | EMA Reward: -0.571 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (101/15000) | EMA Reward: -0.575 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (102/15000) | EMA Reward: -0.559 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (103/15000) | EMA Reward: -0.554 | Reward: 0.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (104/15000) | EMA Reward: -0.558 | Reward: -1.0 | Loss: 0.0809 | Steps: 1 | Worker: 0\n",
            "Episode: (105/15000) | EMA Reward: -0.543 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (106/15000) | EMA Reward: -0.547 | Reward: -1.0 | Loss: -0.358 | Steps: 1 | Worker: 0\n",
            "Episode: (107/15000) | EMA Reward: -0.552 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (108/15000) | EMA Reward: -0.556 | Reward: -1.0 | Loss: -0.167 | Steps: 1 | Worker: 0\n",
            "Episode: (109/15000) | EMA Reward: -0.561 | Reward: -1.0 | Loss: 0.228 | Steps: 1 | Worker: 1\n",
            "Episode: (110/15000) | EMA Reward: -0.565 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (111/15000) | EMA Reward: -0.549 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (112/15000) | EMA Reward: -0.534 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (113/15000) | EMA Reward: -0.539 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (114/15000) | EMA Reward: -0.543 | Reward: -1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (115/15000) | EMA Reward: -0.538 | Reward: 0.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (116/15000) | EMA Reward: -0.522 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 0\n",
            "Episode: (117/15000) | EMA Reward: -0.507 | Reward: 1.0 | Loss: -inf | Steps: 0 | Worker: 1\n",
            "Episode: (118/15000) | EMA Reward: -0.492 | Reward: 1.0 | Loss: 0.31 | Steps: 1 | Worker: 0\n",
            "Episode: (119/15000) | EMA Reward: -0.497 | Reward: -1.0 | Loss: 0.0991 | Steps: 1 | Worker: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qXbSqOOLAKd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "35be749a-153f-4aa2-f466-bf30f9067f5b"
      },
      "source": [
        "runner.play(plays=100, max_steps_per_episode=max_steps_per_episode, show_plots=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg rewards: -0.52 +/- 0.84\n",
            "Avg steps: 1.55 +/- 0.84\n",
            "Avg entropy: 0.5699999928474426 +/- 0.10999999940395355\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RoY6P3OzRyy"
      },
      "source": [
        "## Save model weights for a later use (optional)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We2l8xxlznvz"
      },
      "source": [
        "episode_count = A3CAgent.global_episode\n",
        "if save_model:\n",
        "    model_folder = os.path.join(\"./models\", gym_name, implementation, \"Ep_\" + str(episode_count).zfill(5), \"model\")\n",
        "    if not os.path.exists(model_folder):\n",
        "        os.makedirs(model_folder)\n",
        "    neural_nets[-1].save_weights(filepath=model_folder, save_format=\"tf\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}